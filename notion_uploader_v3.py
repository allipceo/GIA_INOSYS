#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GIA_INFOSYS Notion í†µí•© ì—…ë¡œë” ìŠ¤í¬ë¦½íŠ¸ (v3)
Phase 2-2, ê³¼ì—… 1: ì§€ì‹ ê·¸ë˜í”„ ë° ì¸ì‚¬ì´íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•

ì‘ì„±ì¼: 2025ë…„ 8ì›” 22ì¼
ì‘ì„±ì: ì„œëŒ€ë¦¬ (Lead Developer)
ëª©ì : ë¬¸ì„œ íŒŒì‹±, LLM ì˜ë¯¸ ì¶”ì¶œ, ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•, ì¸ì‚¬ì´íŠ¸ ìƒì„±ì„ í†µí•©í•˜ëŠ” íŒŒì´í”„ë¼ì¸
"""

import os
import sys
from datetime import datetime
from typing import Optional, Dict, Any, List

# í™˜ê²½ ë³€ìˆ˜
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì¸ì½”ë”© ì˜¤ë¥˜ ë¬´ì‹œ)
try:
    load_dotenv(encoding='utf-8')
except Exception as e:
    logging.warning(f"Failed to load .env with utf-8, trying default: {e}")
    load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('notion_uploader_v3.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class NotionUploaderV3:
    """Notion í†µí•© ì—…ë¡œë” í´ë˜ìŠ¤ (v3) - ì§€ì‹ ê·¸ë˜í”„ ê¸°ëŠ¥ ì¶”ê°€"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.notion_token = os.getenv('NOTION_TOKEN')
        self.notion_database_id = os.getenv('NOTION_DATABASE_ID')
        self.max_retry_attempts = int(os.getenv('MAX_RETRY_ATTEMPTS', 3))
        self.request_timeout = int(os.getenv('REQUEST_TIMEOUT', 30))

        # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì„¤ì •
        self.notion_simulation_mode = True  # í™˜ê²½ ë¬¸ì œë¡œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
        self.parser_simulation_mode = True
        
        logger.info("Notion ì—…ë¡œë” v3 ì´ˆê¸°í™” ì™„ë£Œ (ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ)")

        # ê°œì²´ ì •ë³´ DB ID (ì‹œë®¬ë ˆì´ì…˜ìš©)
        self.entity_database_id = "entity_db_sim_" + datetime.now().strftime('%Y%m%d_%H%M%S')

    def create_entity_database(self) -> Dict[str, Any]:
        """ê°œì²´ ì •ë³´ DB ìƒì„± ì‹œë®¬ë ˆì´ì…˜"""
        logger.info("=== ê°œì²´ ì •ë³´ DB ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ===")
        
        db_structure = {
            "database_name": "ê°œì²´ ì •ë³´ DB",
            "workspace": "GIA_INFOSYS",
            "properties": {
                "ì´ë¦„": {"type": "Title", "description": "ê°œì²´ì˜ ì´ë¦„"},
                "ìœ í˜•": {"type": "Select", "options": ["ì¸ë¬¼", "ê¸°ì—…", "ê¸°ìˆ ", "ì •ì±…", "ì´ë²¤íŠ¸", "ìœ„í—˜", "í”„ë¡œì íŠ¸"]},
                "ê´€ë ¨ ë¬¸ì„œ": {"type": "Relation", "target_db": "ì„ì‹œ í…ŒìŠ¤íŠ¸ìš© DB"},
                "ë¬¸ì„œ ê°œìˆ˜": {"type": "Rollup", "function": "count", "relation_property": "ê´€ë ¨ ë¬¸ì„œ"},
                "ìƒì„±ì¼": {"type": "Date"},
                "ìµœê·¼ ì—…ë°ì´íŠ¸": {"type": "Date"}
            },
            "status": "created",
            "created_at": datetime.now().isoformat()
        }
        
        logger.info(f"ê°œì²´ ì •ë³´ DB ìƒì„± ì„±ê³µ: {db_structure['database_name']}")
        return {"success": True, "database_id": self.entity_database_id, "database_name": db_structure['database_name']}

    def extract_entities_from_text(self, text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ ì¶”ì¶œ (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info("í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ ì¶”ì¶œ ì‹œì‘")
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ê°œì²´ ì¶”ì¶œ
        extracted_entities = [
            {"name": "ì¡°ëŒ€í‘œ", "type": "ì¸ë¬¼", "confidence": 0.95},
            {"name": "GIA_INFOSYS", "type": "í”„ë¡œì íŠ¸", "confidence": 0.90},
            {"name": "Notion", "type": "ê¸°ìˆ ", "confidence": 0.85},
            {"name": "AI ê¸°ìˆ ", "type": "ê¸°ìˆ ", "confidence": 0.80},
            {"name": "ê°œì¸ì •ë³´ì‹œìŠ¤í…œ", "type": "í”„ë¡œì íŠ¸", "confidence": 0.75}
        ]
        
        logger.info(f"ê°œì²´ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_entities)}ê°œ ê°œì²´ ë°œê²¬")
        return extracted_entities

    def generate_advanced_insights(self, entities: List[str], document_content: str) -> List[Dict[str, Any]]:
        """ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info("ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")
        
        insights = []
        
        if "ì¡°ëŒ€í‘œ" in entities and "GIA_INFOSYS" in entities:
            insights.append({
                "type": "ì—°ê´€ì„± ë¶„ì„",
                "content": "ì¡°ëŒ€í‘œë‹˜ì˜ GIA_INFOSYS í”„ë¡œì íŠ¸ëŠ” ê°œì¸ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì˜ í•µì‹¬ìœ¼ë¡œ, AI ê¸°ìˆ ê³¼ Notion ê²°í•©ì˜ í˜ì‹ ì  ì ‘ê·¼ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
                "confidence": 0.85
            })
        
        if "Notion" in entities and "AI ê¸°ìˆ " in entities:
            insights.append({
                "type": "ì‹œì¥ ì˜í–¥ ë¶„ì„",
                "content": "Notionê³¼ AI ê¸°ìˆ ì˜ ê²°í•©ì€ ì§€ì‹ ê´€ë¦¬ ì‹œì¥ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ë©°, ìë™í™”ëœ ì¸ì‚¬ì´íŠ¸ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "confidence": 0.80
            })
        
        if "ê°œì¸ì •ë³´ì‹œìŠ¤í…œ" in entities:
            insights.append({
                "type": "ì „ëµì  ì œì•ˆ",
                "content": "ë¶„ì‚°ëœ ì •ë³´ë¥¼ í†µí•©í•˜ê³  ì˜ë¯¸ìˆëŠ” ì—°ê²°ì„ í†µí•´ ì§€ì‹ìœ¼ë¡œ ì „í™˜í•˜ëŠ” ì‹œìŠ¤í…œì€ ê²½ìŸ ìš°ìœ„ í™•ë³´ì— í•µì‹¬ì…ë‹ˆë‹¤.",
                "confidence": 0.75
            })
        
        logger.info(f"ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {len(insights)}ê°œ")
        return insights

    def process_document_with_knowledge_graph(self, file_path: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬"""
        logger.info(f"ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ë¬¸ì„œ ë‚´ìš©
        simulated_content = f"ì´ê²ƒì€ {doc_type} íŒŒì¼ì˜ ì‹œë®¬ë ˆì´ì…˜ëœ ë‚´ìš©ì…ë‹ˆë‹¤. GIA_INFOSYS í”„ë¡œì íŠ¸ì™€ ê´€ë ¨ëœ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        
        # ê°œì²´ ì¶”ì¶œ
        entities = self.extract_entities_from_text(simulated_content)
        entity_names = [e["name"] for e in entities]
        
        # ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        advanced_insights = self.generate_advanced_insights(entity_names, simulated_content)
        
        return {
            "title": os.path.basename(file_path),
            "doc_type": doc_type,
            "content": simulated_content,
            "keywords": ["GIA_INFOSYS", "ì§€ì‹ ê·¸ë˜í”„", "ì¸ì‚¬ì´íŠ¸"],
            "summary": "ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì…ë‹ˆë‹¤.",
            "entities": entity_names,
            "entity_details": entities,
            "advanced_insights": advanced_insights
        }

    def add_document_to_notion_v3(self, document_data: Dict[str, Any]) -> str:
        """Notion ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜ (v3)"""
        page_id = f"sim_page_v3_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(document_data.get('title', '')) % 10000}"
        
        logger.info(f"ë¬¸ì„œ ì¶”ê°€ ì„±ê³µ (ì‹œë®¬ë ˆì´ì…˜ v3): {document_data.get('title')} (ID: {page_id})")
        logger.info(f"  - ë¬¸ì„œ ìœ í˜•: {document_data.get('doc_type')}")
        logger.info(f"  - ê°œì²´ ìˆ˜: {len(document_data.get('entity_details', []))}")
        logger.info(f"  - ê³ ê¸‰ ì¸ì‚¬ì´íŠ¸ ìˆ˜: {len(document_data.get('advanced_insights', []))}")
        
        return page_id

    def run_knowledge_graph_pipeline(self, test_files_dir: str = "./test_files") -> Dict[str, Any]:
        """ì§€ì‹ ê·¸ë˜í”„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (v3)"""
        logger.info("=== ì§€ì‹ ê·¸ë˜í”„ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (v3) ===")
        
        # 1. ê°œì²´ ì •ë³´ DB ìƒì„±
        db_result = self.create_entity_database()
        
        # 2. í…ŒìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
        test_files = [
            {"path": "test.docx", "type": "docx"},
            {"path": "test.pptx", "type": "pptx"},
            {"path": "test.pdf", "type": "pdf"},
        ]
        
        results = []
        success_count = 0
        total_entities = 0
        total_insights = 0
        
        for file_info in test_files:
            file_path = file_info['path']
            file_type = file_info['type']
            
            logger.info(f"--- íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path} ---")
            
            processed_data = self.process_document_with_knowledge_graph(file_path, file_type)
            if processed_data:
                page_id = self.add_document_to_notion_v3(processed_data)
                results.append({
                    "file": file_path,
                    "type": file_type,
                    "success": True,
                    "page_id": page_id,
                    "entities_count": len(processed_data.get('entity_details', [])),
                    "insights_count": len(processed_data.get('advanced_insights', []))
                })
                success_count += 1
                total_entities += len(processed_data.get('entity_details', []))
                total_insights += len(processed_data.get('advanced_insights', []))
        
        overall_success = success_count == len(test_files)
        
        logger.info(f"=== ì§€ì‹ ê·¸ë˜í”„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (v3) ===")
        logger.info(f"ì„±ê³µ: {success_count}/{len(test_files)}, ì´ ê°œì²´: {total_entities}, ì´ ì¸ì‚¬ì´íŠ¸: {total_insights}")
        
        return {
            "success": overall_success,
            "database_created": db_result.get('success'),
            "total_files": len(test_files),
            "success_count": success_count,
            "total_entities": total_entities,
            "total_insights": total_insights,
            "results": results
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== GIA_INFOSYS Notion í†µí•© ì—…ë¡œë” (v3) - ì§€ì‹ ê·¸ë˜í”„ ì‹œì‘ ===")
    
    uploader = NotionUploaderV3()
    results = uploader.run_knowledge_graph_pipeline()
    
    print("\n=== ì§€ì‹ ê·¸ë˜í”„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (v3) ===")
    for res in results['results']:
        status = "âœ… ì„±ê³µ" if res['success'] else "âŒ ì‹¤íŒ¨"
        print(f"{res['file']} ({res['type']}): {status}")
        if res['success']:
            print(f"  - í˜ì´ì§€ ID: {res['page_id']}")
            print(f"  - ê°œì²´ ìˆ˜: {res['entities_count']}")
            print(f"  - ì¸ì‚¬ì´íŠ¸ ìˆ˜: {res['insights_count']}")
    
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"  - ì´ íŒŒì¼: {results['total_files']}")
    print(f"  - ì„±ê³µ: {results['success_count']}")
    print(f"  - ì´ ê°œì²´: {results['total_entities']}")
    print(f"  - ì´ ì¸ì‚¬ì´íŠ¸: {results['total_insights']}")
    
    print(f"\nìµœì¢… ê²°ê³¼: {'ğŸ‰ ì§€ì‹ ê·¸ë˜í”„ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!' if results['success'] else 'âš ï¸ ì¼ë¶€ ì²˜ë¦¬ ì‹¤íŒ¨!'}")
    
    return results['success']

if __name__ == "__main__":
    main()
